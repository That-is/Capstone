{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CgcrSXGBXut3"
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06sWPgT-Xw1n"
   },
   "outputs": [],
   "source": [
    "data_006 = pd.read_csv('/content/drive/capstone dataset/cleaned_2018_06.csv')\n",
    "data_007 = pd.read_csv('/content/drive/capstone dataset/cleaned_2018_07.csv')\n",
    "data_008 = pd.read_csv('/content/drive/capstone dataset/cleaned_2018_08.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data_006[data_006['tpep_pickup_datetime']>'2018-06-30 23:46:00']\n",
    "data2 = data_007[data_007['tpep_pickup_datetime']<'2018-07-01 00:14:00']\n",
    "data3 = data_007[data_007['tpep_pickup_datetime']>'2018-07-31 23:46:00']\n",
    "data4 = data_008[data_008['tpep_pickup_datetime']<'2018-08-01 00:14:00']\n",
    "data_add_06 = pd.concat([data1,data2])\n",
    "data_add_07 = pd.concat([data3,data4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TBF4uO2WvEj"
   },
   "outputs": [],
   "source": [
    "def minutes_trans(newData):\n",
    "  newData['tpep_pickup_datetime'] = newData['tpep_pickup_datetime'].map(lambda x: parse(x).strftime('%Y/%m/%d %H:%M'))\n",
    "  newData['tpep_pickup_datetime'] = newData['tpep_pickup_datetime'].map(lambda x: datetime.strptime(x,'%Y/%m/%d %H:%M'))\n",
    "  return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwPPoyjxaiow"
   },
   "outputs": [],
   "source": [
    "#transfer data of each month\n",
    "newData = minutes_trans(data_008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer appended last 15minutes \n",
    "newData_06_append = minutes_trans(data_add_06)\n",
    "newData_06_append.reset_index(drop=True,inplace=True)\n",
    "newData_07_append = minutes_trans(data_add_07)\n",
    "newData_07_append.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ziXPRE4MWvEn"
   },
   "outputs": [],
   "source": [
    "# generate time series\n",
    "def generate_countinous(start ='6/01/2018', end ='7/1/2018'):\n",
    "  test_date = pd.date_range(start,end, freq='T')\n",
    "  test_date = pd.DataFrame(test_date)\n",
    "  test_date.rename(columns = {0:'pickup_time_g'}, inplace = True)\n",
    "  test_date.drop([len(test_date)-1], inplace=True)\n",
    "  return test_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ntL4zj-4i1uh"
   },
   "outputs": [],
   "source": [
    "#test_date = generate_countinous('6/01/2018','7/01/2018')\n",
    "test_date = generate_countinous('8/01/2018','9/01/2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_date = generate_countinous('7/31/2018 23:46:00 ','8/01/2018 00:14:00')\n",
    "test_date_06 = generate_countinous('6/30/2018 23:46:00 ','7/01/2018 00:14:00')\n",
    "test_date_07 = generate_countinous('7/31/2018 23:46:00 ','8/01/2018 00:14:00')\n",
    "#test_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDl5Xqx-i2Ww"
   },
   "outputs": [],
   "source": [
    "#extract the data slice that are wanted\n",
    "#### really important stage for extraction ---- later\n",
    "def data_extract(data, number):\n",
    "  data_index = data[data['PULocationID'] == number].index.to_list()\n",
    "  newData = data.iloc[data_index,:]\n",
    "  return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97YW-PRXjAwE"
   },
   "outputs": [],
   "source": [
    "cleaned_data = data_extract(newData, 237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6ICgJ2JlnIA"
   },
   "outputs": [],
   "source": [
    "### try\n",
    "def minutes_accumulate(cleaned_data):\n",
    "  Counted_value = pd.DataFrame(cleaned_data.groupby('tpep_pickup_datetime', as_index=False).size())\n",
    "  Counted_value.reset_index(drop= False, inplace = True)\n",
    "  Counted_value.rename(columns = {0:'minute_demand'}, inplace = True)\n",
    "  Counted_value['Sum_price'] = pd.Series(cleaned_data.groupby('tpep_pickup_datetime')['total_amount'].sum().values)\n",
    "  Counted_value['Mean_price'] = pd.Series(cleaned_data.groupby('tpep_pickup_datetime')['total_amount'].mean().values)\n",
    "  Counted_value['Sum_duration'] = pd.Series(cleaned_data.groupby('tpep_pickup_datetime')['duration'].sum().values)\n",
    "  Counted_value['Mean_duration'] = pd.Series(cleaned_data.groupby('tpep_pickup_datetime')['duration'].mean().values)\n",
    "  return Counted_value\n",
    "#all(Counted_value.index == avg_price.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCpqvMTpphiM"
   },
   "outputs": [],
   "source": [
    "# merge the result and macth in minutes\n",
    "cleaned_data = minutes_accumulate(cleaned_data)\n",
    "matched_date = pd.merge(test_date, cleaned_data, left_on = 'pickup_time_g', right_on = 'tpep_pickup_datetime', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtLzgoFSp2X4"
   },
   "outputs": [],
   "source": [
    "# if there is no order at this moment, we set this value to 999\n",
    "#remove and fill in null values\n",
    "def null_value_implaint(matched_date):\n",
    "  del matched_date['tpep_pickup_datetime']\n",
    "  matched_date['minute_demand'] = matched_date['minute_demand'].fillna(0)\n",
    "  matched_date['Sum_price'] = matched_date['Sum_price'].fillna(0)\n",
    "  matched_date['Mean_price'] = matched_date['Mean_price'].fillna(0)\n",
    "  matched_date['Sum_duration'] = matched_date['Sum_duration'].fillna(0)\n",
    "  matched_date['Mean_duration'] = matched_date['Mean_duration'].fillna(0)\n",
    "  return matched_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uhaTMYZ08D_"
   },
   "outputs": [],
   "source": [
    "matched_date = null_value_implaint(matched_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lW4X7oJuWvEu"
   },
   "outputs": [],
   "source": [
    "def interval_extraction(matched_date, window_num):\n",
    "    Demand = pd.Series(matched_date['minute_demand']).rolling(window = window_num).sum()\n",
    "    Price = pd.Series(matched_date['Sum_price']).rolling(window = window_num).sum()\n",
    "    Duration = pd.Series(matched_date['Sum_duration']).rolling(window = window_num).sum()\n",
    "    #check the number of null values\n",
    "    #tryit.isna().sum()\n",
    "    Demand.dropna(inplace=True)\n",
    "    Price.dropna(inplace=True)\n",
    "    Duration.dropna(inplace=True)\n",
    "    Demand.reset_index(drop=True,inplace = True)\n",
    "    Price.reset_index(drop=True,inplace = True)\n",
    "    Duration.reset_index(drop=True,inplace = True)\n",
    "    cleaned_data = pd.concat([Demand,Price,Duration], axis=1)\n",
    "    cleaned_data.rename(columns = {'minute_demand':'15_demand','Sum_price':'15_sum_price','Sum_duration':'15_sum_duration'},inplace = True)\n",
    "    new = pd.concat([matched_date,cleaned_data], axis=1)\n",
    "    new.dropna(inplace=True)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUVcZbTcu9bc"
   },
   "outputs": [],
   "source": [
    "new = interval_extraction(matched_date, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzDaVqTGWvE2"
   },
   "outputs": [],
   "source": [
    "def separate_timestamp(matched_date):\n",
    "    matched_date['weekday'] = matched_date['pickup_time_g'].dt.weekday\n",
    "    matched_date['month'] = matched_date['pickup_time_g'].map(lambda x: x.month)\n",
    "    matched_date['hour'] = matched_date['pickup_time_g'].map(lambda x: x.hour)\n",
    "    matched_date['minute'] = matched_date['pickup_time_g'].map(lambda x: x.minute)\n",
    "    matched_date['second'] = matched_date['pickup_time_g'].map(lambda x: x.second)\n",
    "    matched_date['day'] = matched_date['pickup_time_g'].map(lambda x: x.day)\n",
    "    return matched_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xb3A-9FyWvE5"
   },
   "outputs": [],
   "source": [
    "matched_date = separate_timestamp(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRZaQmUrWvE7"
   },
   "outputs": [],
   "source": [
    "def num_attach(dataset,number):\n",
    "  locationID = [number] * len(matched_date)\n",
    "  matched_date['locationID'] = pd.Series(locationID)\n",
    "  return matched_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2r9UZUjq3sSZ"
   },
   "outputs": [],
   "source": [
    "cluster0_06 = [4, 11, 12, 14, 17, 21, 22, 24, 25, 26, 29, 33, 34, 35, 36, 37, 39, 40, 41, 42, 45, 49, 52, 54, 55, 61, 62, 63, 65, 66, 67, 71, 72, 74, 75, 76, 77, 80, 85, 88, 89, 91, 97, 106, 108, 111, 112, 116, 120, 123, 125, 127, 128, 133, 149, 150, 152, 153, 154, 155, 165, 166, 177, 178, 181, 188, 189, 190, 194, 195, 202, 209, 210, 217, 222, 224, 225, 227, 228, 232, 243, 244, 255, 256, 257, 261]\n",
    "cluster1_06 = [13, 43, 50, 87, 90, 100, 113, 114, 137, 140, 141, 143, 144, 148, 151, 158, 211, 229, 231, 233, 238, 246, 249, 262, 263]\n",
    "cluster2_06 = [48, 68, 79, 107, 142, 161, 162, 163, 164, 170, 186, 230, 234, 236, 237, 239]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_or_not = pd.unique(newData['PULocationID'])\n",
    "cluster0_06_in = list()\n",
    "cluster0_06_out = list()\n",
    "cluster1_06_in = list()\n",
    "#void\n",
    "cluster1_06_out = list()\n",
    "cluster2_06_in = list()\n",
    "#void \n",
    "cluster2_06_out = list()\n",
    "for i in cluster0_06:\n",
    "  if i in matched_or_not:\n",
    "    cluster0_06_in.append(i)\n",
    "  else:\n",
    "    cluster0_06_out.append(i)\n",
    "for i in cluster1_06:\n",
    "    if i in matched_or_not:\n",
    "       cluster1_06_in.append(i)\n",
    "    else:\n",
    "      cluster1_06_out.append(i)\n",
    "for i in cluster2_06:\n",
    "  if i in matched_or_not:\n",
    "    cluster2_06_in.append(i)\n",
    "  else:\n",
    "    cluster2_06_out.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones = pd.read_csv('/Users/liuyang/Desktop/capstone/capstone_dataset/Geo_taxizone.csv')\n",
    "taxi_zones = taxi_zones[['zone','borough','LocationID', 'longitude','latitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7RaEg4aIzEMf",
    "outputId": "90f57b9f-11b9-4543-b8f4-e7a5ed2e7729"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.72039818763733\n"
     ]
    }
   ],
   "source": [
    "def final_function(dataset, cluster_name,test_date):\n",
    "  start = time.time()\n",
    "  final_result = pd.DataFrame(columns = ['pickup_time_g', 'minute_demand', 'Sum_price', 'Mean_price',\n",
    "       'Sum_duration', 'Mean_duration', '15_demand', '15_sum_price',\n",
    "       '15_sum_duration', 'weekday', 'month', 'hour', 'minute', 'second',\n",
    "       'day', 'locationID'])\n",
    "  for number in cluster_name:   \n",
    "    cleaned_data = data_extract(dataset, number)\n",
    "    cleaned_data = minutes_accumulate(cleaned_data)\n",
    "    matched_date = pd.merge(test_date, cleaned_data, left_on = 'pickup_time_g', right_on = 'tpep_pickup_datetime', how = 'left')\n",
    "    matched_date = null_value_implaint(matched_date)\n",
    "    new = interval_extraction(matched_date, 15)\n",
    "    matched_date = separate_timestamp(new)\n",
    "    key1 = num_attach(matched_date, number)\n",
    "    final_result = pd.concat([final_result,key1],ignore_index = True)\n",
    "  try_data1 = pd.Series(final_result['15_sum_duration']/final_result['15_demand'])\n",
    "  final_result['15_avg_duration'] = try_data1.fillna(0)\n",
    "  try_data2 = pd.Series(final_result['15_sum_price']/final_result['15_demand'])\n",
    "  final_result['15_avg_price'] = try_data2.fillna(0)\n",
    "  final_result.drop(columns = ['Sum_duration','Sum_price','15_sum_duration','15_sum_price'], inplace=True)\n",
    "  final_result = pd.merge(final_result, taxi_zones, left_on = 'locationID', right_on = 'LocationID', how = 'left')\n",
    "  final_result.drop(columns = ['locationID'], inplace = True)\n",
    "  end = time.time()\n",
    "  print(end-start)\n",
    "  return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "High_06 = final_function(newData,cluster0_06, test_date)\n",
    "High_06_append = final_function(newData_07_append,cluster2_06_in, test_date_07)\n",
    "Result = pd.concat([High_06,High_06_append], ignore_index=True)\n",
    "Result.to_csv('/content/drive/capstone dataset/High_07.csv')\n",
    "#High_06.to_csv('/content/drive/capstone dataset/High_07.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_columns(cluster, month):\n",
    "    name = ['Low','Medium','High']\n",
    "    test = pd.read_csv(f'/content/drive/capstone dataset/cluster{cluster}_0{month}.csv')\n",
    "    test.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    cleaned = pd.merge(test, taxi_zones, left_on = 'locationID', right_on = 'LocationID', how = 'left')\n",
    "    cleaned.drop(columns = ['locationID'], inplace = True)\n",
    "    cleaned.to_csv(f'/content/drive/capstone dataset/{name[cluster]}_0{month}.csv')\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0,1,2]\n",
    "b = [6,7,8]\n",
    "for i in a:\n",
    "    for j in b:\n",
    "        format_columns(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_summer_data(number):\n",
    "    b = ['LowDemand','MediumDemand','HighDemand']\n",
    "    Low_06 = pd.read_csv(f'/Users/liuyang/Desktop/formated_data/cluster{number}_06.csv')\n",
    "    Low_07 = pd.read_csv(f'/Users/liuyang/Desktop/formated_data/cluster{number}_07.csv')\n",
    "    Low_08 = pd.read_csv(f'/Users/liuyang/Desktop/formated_data/cluster{number}_08.csv')\n",
    "    result = pd.concat(['Low_06','Low_07','Low_08'], ignore_index = True)\n",
    "    result.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    result.to_csv(f'/Users/liuyang/Desktop/{b[number]}_3months.csv')\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3months_format_transform.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
